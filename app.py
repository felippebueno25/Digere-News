import os
import re
import time
import json
import hashlib
from datetime import datetime, timedelta, timezone
import feedparser
from google import genai
from ddgs import DDGS
import requests # Para envio ao Telegram
from curl_cffi import requests as crequests # Bypass de bloqueios (UOL/Globo)
import trafilatura

# ================= CONFIGURA√á√ïES =================
GEMINI_KEY = os.getenv("GEMINI_API_KEY")
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID")

RSS_URL = "https://news.google.com/rss/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRFZxYUdjU0JYQjBMVUpTR2dKQ1VpZ0FQAQ?hl=pt-BR&gl=BR&ceid=BR%3Apt-419"
MAX_ITEMS = 20 # Limite de not√≠cias por execu√ß√£o
HISTORY_FILE = ".news_history.json" # Hist√≥rico de not√≠cias j√° enviadas
HISTORY_DAYS = 7 # Manter hist√≥rico por 7 dias 

# ================= FUN√á√ïES DE APOIO =================

def get_br_time():
    """Hora atual de Bras√≠lia (UTC-3)."""
    return (datetime.now(timezone.utc) - timedelta(hours=3)).strftime('%d/%m %H:%M')

def load_history():
    """Carrega o hist√≥rico de not√≠cias j√° processadas."""
    if not os.path.exists(HISTORY_FILE):
        return {}
    
    try:
        with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception:
        return {}

def save_history(history):
    """Salva o hist√≥rico de not√≠cias processadas."""
    try:
        with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao salvar hist√≥rico: {e}")

def get_news_hash(title, url):
    """
    Gera um hash √∫nico para uma not√≠cia baseado em t√≠tulo + URL.
    Isso evita duplicatas mesmo se o t√≠tulo mudar ligeiramente.
    """
    key = f"{title.lower().strip()}|{url.lower().strip()}"
    return hashlib.md5(key.encode()).hexdigest()

def is_news_duplicate(title, url, history):
    """
    Verifica se a not√≠cia j√° foi processada.
    Retorna True se √© duplicata, False se √© nova.
    """
    news_hash = get_news_hash(title, url)
    return news_hash in history

def clean_old_history(history):
    """
    Remove not√≠cias do hist√≥rico que t√™m mais de HISTORY_DAYS dias.
    Mant√©m o arquivo controlado.
    """
    cutoff_date = (datetime.now(timezone.utc) - timedelta(hours=3, days=HISTORY_DAYS)).isoformat()
    
    cleaned = {}
    for hash_id, entry in history.items():
        if entry.get('timestamp', '') > cutoff_date:
            cleaned[hash_id] = entry
    
    return cleaned

def resolve_url_ddg(title):
    """
    Busca a URL original no DuckDuckGo.
    """
    clean_title = title.split(" - ")[0] if " - " in title else title
    print(f"  [Busca] '{clean_title}'...")

    for attempt in range(1, 3):
        try:
            with DDGS() as ddgs:
                results = list(ddgs.text(clean_title, region='br-pt', max_results=1))
                if results:
                    return results[0]['href']
        except Exception:
            time.sleep(1) 
    return None

def extract_content_robust(url):
    """
    Usa 'curl_cffi' (Chrome 120) para baixar o HTML e 'trafilatura' para extrair o texto.
    """
    if not url or "news.google.com" in url: return None
    
    print("  [Extra√ß√£o] Baixando conte√∫do...")
    try:
        # Impersonate Chrome resolve o erro 403 do UOL
        response = crequests.get(url, impersonate="chrome120", timeout=15)
        
        if response.status_code == 200:
            text = trafilatura.extract(response.content, include_comments=False, include_tables=False)
            if text and len(text) > 200:
                print("  -> Sucesso!")
                return text
            else:
                print("  -> Texto curto/vazio.")
        else:
            print(f"  -> Bloqueado (Status {response.status_code})")
            
    except Exception as e:
        print(f"  -> Erro no download: {e}")
    return None

def generate_final_report(news_data):
    """
    Envia TODAS as not√≠cias para o Gemini de uma vez s√≥, 
    otimizado para economia de tokens.
    """
    if not GEMINI_KEY: return "‚ö†Ô∏è Erro: API Key n√£o configurada."
    if not news_data: return "‚ö†Ô∏è Nenhuma not√≠cia foi coletada."

    print(f"\n[IA] Gerando relat√≥rio consolidado ({len(news_data)} not√≠cias)...")

    # INSTRU√á√ÉO DO SISTEMA (Compactada)
    # Define o formato de sa√≠da desejado e a persona.
    system_instruction = """Persona: Voc√™ √© um Analista de Intelig√™ncia S√™nior com foco em An√°lise de Discurso e Contexto Hist√≥rico. Sua miss√£o n√£o √© apenas informar, mas "desarmar" a not√≠cia. Voc√™ escreve para um cidad√£o exigente que despreza o sensacionalismo e busca entender as engrenagens por tr√°s dos fatos. Sua Miss√£o: Processar not√≠cias brutas e entregar uma an√°lise profunda, √©tica e cr√≠tica. Seu foco √© identificar vieses, interesses ocultos e consequ√™ncias sociais, eliminando o lixo informacional. O que a not√≠cia n√£o est√° dizendo? Quais vozes foram omitidas? Isso √© um evento isolado ou parte de um padr√£o hist√≥rico/pol√≠tico? Quem ganha com a propaga√ß√£o desta narrativa espec√≠fica? etc.Linguagem Humana e Direta: Sem "corporativ√™s". Use um tom de conversa inteligente e honesta. Transpar√™ncia: Se houver ambiguidade na fonte, aponte-a. Concis√£o Cr√≠tica: V√° direto ao ponto, mas n√£o sacrifique a complexidade pelo simplismo. O FATO NU E CRU: (A not√≠cia limpa de adjetivos e manipula√ß√µes). O QUE EST√Å EM JOGO: (Os interesses pol√≠ticos, econ√¥micos ou sociais por tr√°s do evento). ALERTA DE RU√çDO: (Identifique se h√° sensacionalismo, vi√©s ideol√≥gico √≥bvio ou distra√ß√£o de outros temas importantes). PARA PENSAR: (Uma pergunta provocativa ou uma conex√£o com a realidade do leitor que amplia a vis√£o sobre o tema)."""

    # MONTAGEM DO PROMPT (Otimizada)
    prompt_content = f"Data: {get_br_time()}\n\n"
    
    for item in news_data:
        # 1. Limpeza de "sujeira" (espa√ßos duplos e quebras de linha excessivas)
        raw_text = item['content'] or ""
        clean_text = re.sub(r'\s+', ' ', raw_text).strip()
        
        # 2. Truncamento inteligente (2500 chars √© suficiente para o contexto principal)
        # O lead jornal√≠stico est√° sempre no in√≠cio.
        content_preview = clean_text[:2500] 
        
        # 3. Formato de entrada minimalista para economizar tokens
        # O LLM entende XML-like tags ou separadores simples melhor que texto descritivo.
        prompt_content += f"""
        <n>
        <original_title>{item['title']}</original_title>
        <url>{item['url']}</url>
        <body>{clean_text[:2500]}</body>
        </n>
        """

    try:
        client = genai.Client(api_key=GEMINI_KEY)
        response = client.models.generate_content(
            model='gemini-2.5-flash-lite', # Modelo econ√¥mico
            config=genai.types.GenerateContentConfig(
                temperature=0.4 # Menos criativo, mais focado nos fatos
            ),
            contents=[system_instruction, prompt_content]
        )
        return response.text
    except Exception as e:
        return f"Erro fatal na IA: {e}"

def send_telegram(text):
    """Envia o relat√≥rio final para o Telegram."""
    if not TELEGRAM_TOKEN or not TELEGRAM_CHAT_ID: return
    
    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
    
    parts = [text[i:i+4000] for i in range(0, len(text), 4000)]
    
    for part in parts:
        try:
            requests.post(url, json={"chat_id": TELEGRAM_CHAT_ID, "text": part, "parse_mode": "Markdown"})
        except Exception as e:
            print(f"Erro Telegram: {e}")

# ================= MAIN =================

def main():
    print("--- üöÄ Iniciando v8.0 (Com Deduplica√ß√£o de Estado) ---")
    
    # Carregar hist√≥rico de not√≠cias j√° processadas
    history = load_history()
    history = clean_old_history(history)  # Remover entradas antigas
    print(f"üìã Hist√≥rico carregado: {len(history)} not√≠cias j√° processadas")
    
    feed = feedparser.parse(RSS_URL)
    news_buffer = [] 
    duplicates_found = 0
    
    count = 0
    for entry in feed.entries:
        if count >= MAX_ITEMS: break
        
        # Resolver URL antes de verificar duplicata
        clean_url = resolve_url_ddg(entry.title)
        if not clean_url:
            print(f"üì∞ {entry.title}")
            print("   -> Pulei (Sem link)")
            continue
        
        # Verificar duplicata
        if is_news_duplicate(entry.title, clean_url, history):
            print(f"üì∞ {entry.title}")
            print("   -> Duplicata detectada (pulado)")
            duplicates_found += 1
            continue
        
        print(f"üì∞ {entry.title}")
        
        # Extrair Conte√∫do (Camuflado)
        content = extract_content_robust(clean_url)
        
        # Guardar no Buffer
        news_buffer.append({
            'title': entry.title,
            'url': clean_url,
            'content': content
        })
        
        count += 1
        time.sleep(2)
    
    print(f"\nüìä Resumo: {len(news_buffer)} not√≠cias novas, {duplicates_found} duplicatas")

    # Gerar Relat√≥rio Final
    if news_buffer:
        final_report = generate_final_report(news_buffer)
        
        # Atualizar hist√≥rico com as not√≠cias processadas
        now = (datetime.now(timezone.utc) - timedelta(hours=3)).isoformat()
        for item in news_buffer:
            news_hash = get_news_hash(item['title'], item['url'])
            history[news_hash] = {
                'title': item['title'],
                'url': item['url'],
                'timestamp': now
            }
        
        save_history(history)
        
        # Salvar e Enviar
        with open("briefing_diario.md", "w", encoding="utf-8") as f:
            f.write(final_report)
        
        send_telegram(final_report)
        print("\n‚úÖ Relat√≥rio enviado com sucesso!")
    else:
        print("\n‚ö†Ô∏è Nenhuma not√≠cia nova para processar.")

if __name__ == "__main__":
    main()